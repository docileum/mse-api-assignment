{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "974bfb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fileinput import filename\n",
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619eb65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DIR_DATA = Path.cwd().parents[0] / \"data\"\n",
    "DIR_REPORTS_PDF = DIR_DATA / \"mse-daily-reports\"\n",
    "DIR_REPORTS_CSV = DIR_DATA / \"mse-daily-data\"\n",
    "\n",
    "FILE_PDF = DIR_REPORTS_PDF / \"mse-daily-09-05-2025.pdf\"\n",
    "FILE_CSV = DIR_REPORTS_CSV / \"mse-daily-09-05-2025.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a09eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import date, time, datetime\n",
    "from pathlib import Path\n",
    "import pdfplumber\n",
    "\n",
    "# Month map (handles \"Sep\" and \"Sept\")\n",
    "_MONTHS = {\n",
    "    'jan':1,'january':1,'feb':2,'february':2,'mar':3,'march':3,'apr':4,'april':4,\n",
    "    'may':5,'jun':6,'june':6,'jul':7,'july':7,'aug':8,'august':8,\n",
    "    'sep':9,'sept':9,'september':9,'oct':10,'october':10,\n",
    "    'nov':11,'november':11,'dec':12,'december':12\n",
    "}\n",
    "\n",
    "def _mkdate(y, m, d):  # y,m,d may be str\n",
    "    return date(int(y), int(m), int(d))\n",
    "\n",
    "def _norm_text(s: str) -> str:\n",
    "    return re.sub(r'\\s+', ' ', s or '').strip()\n",
    "\n",
    "def _parse_date_str(s: str, day_first: bool = True):\n",
    "    \"\"\"Parse a date from free text. Returns datetime.date or None.\"\"\"\n",
    "    s = _norm_text(s)\n",
    "\n",
    "    # 1) 5 September 2025 | 05 Sep 2025 | 5 Sept, 2025 | 5th September 2025\n",
    "    m = re.search(r'(?i)\\b(\\d{1,2})(?:st|nd|rd|th)?\\s+([A-Za-z]{3,9}),?\\s+(20\\d{2})\\b', s)\n",
    "    if m:\n",
    "        d, mon, y = m.groups()\n",
    "        mon_num = _MONTHS.get(mon.lower())\n",
    "        if mon_num:\n",
    "            return _mkdate(y, mon_num, d)\n",
    "\n",
    "    # 2) September 5, 2025 | Sep 05 2025 | Sept 5th 2025\n",
    "    m = re.search(r'(?i)\\b([A-Za-z]{3,9})\\s+(\\d{1,2})(?:st|nd|rd|th)?,?\\s+(20\\d{2})\\b', s)\n",
    "    if m:\n",
    "        mon, d, y = m.groups()\n",
    "        mon_num = _MONTHS.get(mon.lower())\n",
    "        if mon_num:\n",
    "            return _mkdate(y, mon_num, d)\n",
    "\n",
    "    # 3) ISO-like: 2025-09-05 / 2025/09/05 / 2025.09.05\n",
    "    m = re.search(r'\\b(20\\d{2})[-/.](\\d{1,2})[-/.](\\d{1,2})\\b', s)\n",
    "    if m:\n",
    "        y, mth, d = m.groups()\n",
    "        try: return _mkdate(y, mth, d)\n",
    "        except ValueError: pass\n",
    "\n",
    "    # 4) Numeric: 05-09-2025 | 05/09/2025 | 5.9.2025\n",
    "    m = re.search(r'\\b(\\d{1,2})[-/.](\\d{1,2})[-/.](20\\d{2})\\b', s)\n",
    "    if m:\n",
    "        a, b, y = m.groups()\n",
    "        # day-first by default (MSE style)\n",
    "        d, mth = (a, b) if day_first else (b, a)\n",
    "        try: return _mkdate(y, mth, d)\n",
    "        except ValueError: pass\n",
    "\n",
    "    return None\n",
    "\n",
    "def _parse_time_str(s: str):\n",
    "    \"\"\"Parse a time from free text. Returns datetime.time or None.\"\"\"\n",
    "    s = _norm_text(s)\n",
    "\n",
    "    # 12-hour with seconds or without (e.g., 02:39:52 pm, 2:39 pm)\n",
    "    m = re.search(r'(?i)\\b(\\d{1,2}):(\\d{2})(?::(\\d{2}))?\\s*(am|pm)\\b', s)\n",
    "    if m:\n",
    "        hh, mm, ss, ap = m.groups()\n",
    "        hh, mm, ss = int(hh), int(mm), int(ss or 0)\n",
    "        ap = ap.lower()\n",
    "        if hh == 12: hh = 0\n",
    "        if ap == 'pm': hh += 12\n",
    "        try: return time(hh, mm, ss)\n",
    "        except ValueError: return None\n",
    "\n",
    "    # 24-hour with optional seconds (e.g., 14:39:52 or 14:39)\n",
    "    m = re.search(r'\\b([01]?\\d|2[0-3]):([0-5]\\d)(?::([0-5]\\d))\\b', s)\n",
    "    if m:\n",
    "        hh, mm, ss = map(int, m.groups())\n",
    "        try: return time(hh, mm, ss)\n",
    "        except ValueError: return None\n",
    "\n",
    "    m = re.search(r'\\b([01]?\\d|2[0-3]):([0-5]\\d)\\b', s)\n",
    "    if m:\n",
    "        hh, mm = map(int, m.groups())\n",
    "        try: return time(hh, mm)\n",
    "        except ValueError: return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_print_date_time(pdf_path: str | Path, search_pages: int = 2, day_first: bool = True):\n",
    "    \"\"\"\n",
    "    Extract ONLY the 'Print Date' and 'Print Time' from the PDF text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    {\n",
    "      'date': datetime.date | None,\n",
    "      'time': datetime.time | None,\n",
    "      'raw_date': str | None,  # snippet matched after the label (if any)\n",
    "      'raw_time': str | None\n",
    "    }\n",
    "    \"\"\"\n",
    "    pdf_path = Path(pdf_path)\n",
    "    raw_date_snip = raw_time_snip = None\n",
    "    text = \"\"\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        n = min(max(search_pages, 1), len(pdf.pages))\n",
    "        # Concatenate small chunks (keeps label context)\n",
    "        page_texts = []\n",
    "        for i in range(n):\n",
    "            page_texts.append(pdf.pages[i].extract_text() or \"\")\n",
    "        text = \"\\n\".join(page_texts)\n",
    "\n",
    "    # Prefer labeled fields\n",
    "    m = re.search(r'(?is)Print\\s*Date\\s*:?\\s*([^\\n\\r]+)', text)\n",
    "    if m: raw_date_snip = m.group(1)\n",
    "    m = re.search(r'(?is)Print\\s*Time\\s*:?\\s*([^\\n\\r]+)', text)\n",
    "    if m: raw_time_snip = m.group(1)\n",
    "\n",
    "    d = _parse_date_str(raw_date_snip) if raw_date_snip else _parse_date_str(text)\n",
    "    t = _parse_time_str(raw_time_snip) if raw_time_snip else _parse_time_str(text)\n",
    "\n",
    "    return {'date': d, 'time': t, 'raw_date': (raw_date_snip or None), 'raw_time': (raw_time_snip or None)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d7c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2025-09-05\n",
      "Time: 14:39:52\n"
     ]
    }
   ],
   "source": [
    "# --- Example usage ---\n",
    "info = extract_print_date_time(FILE_PDF)\n",
    "print(\"Date:\", info['date'])\n",
    "print(\"Time:\", info['time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e38f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numeric_clean(val):\n",
    "    \"\"\"\n",
    "    Clean and convert a value to numeric:\n",
    "    - None/empty -> NaN\n",
    "    - (123.45) -> -123.45\n",
    "    - remove commas\n",
    "    \"\"\"\n",
    "    if val is None:\n",
    "        return np.nan\n",
    "    val = str(val).strip()\n",
    "    if val.lower() == \"none\" or val == \"\":\n",
    "        return np.nan\n",
    "    # Handle parentheses as negatives\n",
    "    if val.startswith(\"(\") and val.endswith(\")\"):\n",
    "        val = \"-\" + val[1:-1]\n",
    "    # Remove commas\n",
    "    val = val.replace(\",\", \"\")\n",
    "    try:\n",
    "        return float(val)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def clean_cell(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    x = re.sub(r'\\s+', ' ', str(x)).strip()\n",
    "    x = x.replace('–', '-').replace('—', '-')\n",
    "    return x if x else None\n",
    "\n",
    "def is_numericish(s: Optional[str]) -> bool:\n",
    "    if s is None:\n",
    "        return False\n",
    "    s = str(s).strip().replace(\",\", \"\")\n",
    "    return bool(re.fullmatch(r\"[-+]?(\\d+(\\.\\d+)?|\\.\\d+)(%?)\", s))\n",
    "\n",
    "def is_header_like(row: list) -> bool:\n",
    "    \"\"\"Header-like = many text cells, few numeric cells.\"\"\"\n",
    "    cells = [c for c in row if c is not None and str(c).strip() != \"\"]\n",
    "    if not cells:\n",
    "        return False\n",
    "    num_numeric = sum(1 for c in cells if is_numericish(c))\n",
    "    num_alpha   = sum(1 for c in cells if re.search(r\"[A-Za-z]\", str(c)))\n",
    "    return (num_alpha >= max(1, len(cells)//4)) and (num_numeric / len(cells) <= 0.5)\n",
    "\n",
    "def normalize_to_width(rows: list[list], width: int) -> list[list]:\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        r = list(r)\n",
    "        if len(r) < width:\n",
    "            r = r + [None] * (width - len(r))\n",
    "        elif len(r) > width:\n",
    "            r = r[:width]\n",
    "        out.append(r)\n",
    "    return out\n",
    "\n",
    "def extract_first_table(pdf_path: str | Path,\n",
    "                        out_csv: Optional[str | Path] = None,\n",
    "                        header: Optional[List[str]] = None,\n",
    "                        skip_header_rows: int = 0,\n",
    "                        auto_skip_header_like: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract the first table. If `header` is provided, we will:\n",
    "      - optionally auto-skip any header-like rows at the top\n",
    "      - then force DataFrame columns to `header`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf_path : str | Path\n",
    "    out_csv : str | Path, optional\n",
    "    header : List[str], optional\n",
    "        Hardcoded column names to use.\n",
    "    skip_header_rows : int\n",
    "        Force skipping this many rows from the top of the table before data.\n",
    "    auto_skip_header_like : bool\n",
    "        If True, skip leading header-like rows automatically.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "    \"\"\"\n",
    "    pdf_path = Path(pdf_path)\n",
    "    out_csv  = Path(out_csv) if out_csv else None\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Try a few strategies to find tables\n",
    "            strategies = [\n",
    "                dict(vertical_strategy=\"lines\", horizontal_strategy=\"lines\",\n",
    "                     snap_tolerance=3, join_tolerance=3, edge_min_length=3),\n",
    "                dict(vertical_strategy=\"lines_strict\", horizontal_strategy=\"lines_strict\"),\n",
    "                dict(vertical_strategy=\"text\", horizontal_strategy=\"text\"),\n",
    "            ]\n",
    "            tables = []\n",
    "            for ts in strategies:\n",
    "                try:\n",
    "                    t = page.extract_tables(table_settings=ts) or []\n",
    "                    for raw in t:\n",
    "                        if raw and len(raw) >= 2 and max(len(r) for r in raw) >= 2:\n",
    "                            tables.append(raw)\n",
    "                    if tables:\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            if not tables:\n",
    "                continue\n",
    "\n",
    "            # Use the first table found\n",
    "            raw = tables[0]\n",
    "            rows = [[clean_cell(c) for c in row] for row in raw]\n",
    "            rows = [r for r in rows if any(c for c in r)]\n",
    "            if not rows:\n",
    "                continue\n",
    "\n",
    "            # Decide how many rows to skip from top if header is provided\n",
    "            start_idx = 0\n",
    "            if header:\n",
    "                if auto_skip_header_like:\n",
    "                    # Skip all consecutive header-like rows from the top\n",
    "                    auto_skip = 0\n",
    "                    for r in rows:\n",
    "                        if is_header_like(r):\n",
    "                            auto_skip += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    start_idx = auto_skip\n",
    "                # Ensure at least skip_header_rows are skipped\n",
    "                start_idx = max(start_idx, skip_header_rows)\n",
    "                cols = list(header)\n",
    "            else:\n",
    "                # Fallback: auto-detect header = first non-empty row\n",
    "                detected = rows[0]\n",
    "                start_idx = 1\n",
    "                cols = []\n",
    "                seen = {}\n",
    "                for i, name in enumerate(detected):\n",
    "                    name = name or f\"col_{i+1}\"\n",
    "                    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "                    if name in seen:\n",
    "                        seen[name] += 1\n",
    "                        name = f\"{name}_{seen[name]}\"\n",
    "                    else:\n",
    "                        seen[name] = 1\n",
    "                    cols.append(name)\n",
    "\n",
    "            # Build DataFrame\n",
    "            data_rows = normalize_to_width(rows[start_idx:], len(cols))\n",
    "            df = pd.DataFrame(data_rows, columns=cols).dropna(how=\"all\")\n",
    "\n",
    "            # Drop last row as it contains weighted averages \n",
    "            df = df.iloc[:-1] if len(df) > 1 else df\n",
    "\n",
    "            # Rearrange columns\n",
    "            cols = ['counter', 'daily_range_high', 'daily_range_low', \n",
    "                    'buy', 'sell', 'previous_closing_price', 'today_closing_price',\n",
    "                      'volume_traded', 'dividend_mk', 'dividend_yield_pct',\n",
    "                      'earnings_yield_pct', 'pe_ratio', 'pbv_ratio', 'market_capitalization_mkmn',\n",
    "                      'profit_after_tax_mkmn', 'num_shares_issue']\n",
    "            df = df[cols]\n",
    "\n",
    "            # Convert to numeric where possible\n",
    "            for c in df.columns:\n",
    "                if c != \"counter\":  # leave counter as string\n",
    "                    df[c] = df[c].apply(to_numeric_clean)\n",
    "\n",
    "            if out_csv:\n",
    "                df.to_csv(out_csv, index=False)\n",
    "                print(f\"✅ First table extracted and saved to {out_csv}\")\n",
    "            return df\n",
    "\n",
    "    print(\"⚠️ No table found in PDF.\")\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c543b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = ['ser_no', 'daily_range_high', 'daily_range_low', \n",
    "         'counter', 'buy', 'sell', 'previous_closing_price', \n",
    "        'today_closing_price', 'volume_traded', 'dividend_mk', 'dividend_yield_pct',\n",
    "        'earnings_yield_pct', 'pe_ratio', 'pbv_ratio', 'market_capitalization_mkmn',\n",
    "        'profit_after_tax_mkmn', 'num_shares_issue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14924d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function\n",
    "df = extract_first_table(FILE_PDF, header=COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8dbc3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(df: pd.DataFrame, value_col: str, weight_col: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute a weighted average for any numeric column in a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the data.\n",
    "    value_col : str\n",
    "        Column name with the values of interest (e.g., 'dividend_mk', 'dividend_pct').\n",
    "    weight_col : str\n",
    "        Column name with the weights (e.g., 'volume_traded').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Weighted average of the value_col, or NaN if weights sum to 0.\n",
    "    \"\"\"\n",
    "    values = df[value_col].astype(float)\n",
    "    weights = df[weight_col].astype(float)\n",
    "\n",
    "    total_weight = weights.sum()\n",
    "    if total_weight == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return (values * weights).sum() / total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bd460ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "counter                        object\n",
       "daily_range_high              float64\n",
       "daily_range_low               float64\n",
       "buy                           float64\n",
       "sell                          float64\n",
       "previous_closing_price        float64\n",
       "today_closing_price           float64\n",
       "volume_traded                 float64\n",
       "dividend_mk                   float64\n",
       "dividend_yield_pct            float64\n",
       "earnings_yield_pct            float64\n",
       "pe_ratio                      float64\n",
       "pbv_ratio                     float64\n",
       "market_capitalization_mkmn    float64\n",
       "profit_after_tax_mkmn         float64\n",
       "num_shares_issue              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cbe0eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted average of dividend_mk: 8.2070\n",
      "Weighted average of dividend_yield_pct: 0.3297\n",
      "Weighted average of earnings_yield_pct: 2.9126\n",
      "Weighted average of pe_ratio: 40.0991\n",
      "Weighted average of pbv_ratio: 12.7717\n",
      "Weighted average of market_capitalization_mkmn: 1119128.5596\n",
      "Weighted average of profit_after_tax_mkmn: 27990.2145\n",
      "Weighted average of num_shares_issue: 9210336168.6222\n"
     ]
    }
   ],
   "source": [
    "cols_weighted = ['dividend_mk', 'dividend_yield_pct', 'earnings_yield_pct',\n",
    "                 'pe_ratio', 'pbv_ratio', 'market_capitalization_mkmn',\n",
    "                 'profit_after_tax_mkmn', 'num_shares_issue']\n",
    "\n",
    "for col in cols_weighted:\n",
    "    wa = weighted_average(df, col, 'volume_traded')\n",
    "    print(f\"Weighted average of {col}: {wa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ef0bbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter</th>\n",
       "      <th>daily_range_high</th>\n",
       "      <th>daily_range_low</th>\n",
       "      <th>buy</th>\n",
       "      <th>sell</th>\n",
       "      <th>previous_closing_price</th>\n",
       "      <th>today_closing_price</th>\n",
       "      <th>volume_traded</th>\n",
       "      <th>dividend_mk</th>\n",
       "      <th>dividend_yield_pct</th>\n",
       "      <th>earnings_yield_pct</th>\n",
       "      <th>pe_ratio</th>\n",
       "      <th>pbv_ratio</th>\n",
       "      <th>market_capitalization_mkmn</th>\n",
       "      <th>profit_after_tax_mkmn</th>\n",
       "      <th>num_shares_issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AIRTEL</td>\n",
       "      <td>138.83</td>\n",
       "      <td>138.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>138.83</td>\n",
       "      <td>138.84</td>\n",
       "      <td>138.82</td>\n",
       "      <td>53640.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.44</td>\n",
       "      <td>2.80</td>\n",
       "      <td>35.74</td>\n",
       "      <td>47.55</td>\n",
       "      <td>1527020.00</td>\n",
       "      <td>42722.11</td>\n",
       "      <td>1.100000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BHL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.55</td>\n",
       "      <td>-64.36</td>\n",
       "      <td>1.36</td>\n",
       "      <td>88173.82</td>\n",
       "      <td>-1370.11</td>\n",
       "      <td>5.878255e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FDHB</td>\n",
       "      <td>619.93</td>\n",
       "      <td>619.87</td>\n",
       "      <td>619.84</td>\n",
       "      <td>619.87</td>\n",
       "      <td>619.97</td>\n",
       "      <td>619.90</td>\n",
       "      <td>133897.0</td>\n",
       "      <td>4.73</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.73</td>\n",
       "      <td>57.76</td>\n",
       "      <td>43.92</td>\n",
       "      <td>4277949.27</td>\n",
       "      <td>74063.00</td>\n",
       "      <td>6.901031e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FMBCH</td>\n",
       "      <td>1650.41</td>\n",
       "      <td>1650.40</td>\n",
       "      <td>1650.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1650.40</td>\n",
       "      <td>1650.41</td>\n",
       "      <td>8900.0</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.91</td>\n",
       "      <td>34.31</td>\n",
       "      <td>12.33</td>\n",
       "      <td>4057120.38</td>\n",
       "      <td>118254.74</td>\n",
       "      <td>2.458250e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICON</td>\n",
       "      <td>17.95</td>\n",
       "      <td>17.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.95</td>\n",
       "      <td>17.95</td>\n",
       "      <td>17.95</td>\n",
       "      <td>42822.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.62</td>\n",
       "      <td>20.37</td>\n",
       "      <td>4.91</td>\n",
       "      <td>0.82</td>\n",
       "      <td>119906.00</td>\n",
       "      <td>24424.49</td>\n",
       "      <td>6.680000e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ILLOVO</td>\n",
       "      <td>1791.42</td>\n",
       "      <td>1791.42</td>\n",
       "      <td>1791.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1791.41</td>\n",
       "      <td>1791.42</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.77</td>\n",
       "      <td>56.47</td>\n",
       "      <td>8.59</td>\n",
       "      <td>1278078.55</td>\n",
       "      <td>22632.00</td>\n",
       "      <td>7.134444e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MPICO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.51</td>\n",
       "      <td>19.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2.20</td>\n",
       "      <td>19.05</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.69</td>\n",
       "      <td>44834.91</td>\n",
       "      <td>8540.17</td>\n",
       "      <td>2.298047e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NBM</td>\n",
       "      <td>8200.00</td>\n",
       "      <td>7919.95</td>\n",
       "      <td>7919.95</td>\n",
       "      <td>7919.94</td>\n",
       "      <td>7919.93</td>\n",
       "      <td>8196.77</td>\n",
       "      <td>122414.0</td>\n",
       "      <td>126.35</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.67</td>\n",
       "      <td>37.42</td>\n",
       "      <td>14.26</td>\n",
       "      <td>3827332.06</td>\n",
       "      <td>102283.00</td>\n",
       "      <td>4.669317e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NBS</td>\n",
       "      <td>1024.89</td>\n",
       "      <td>1024.83</td>\n",
       "      <td>1024.84</td>\n",
       "      <td>1024.85</td>\n",
       "      <td>1024.91</td>\n",
       "      <td>1024.86</td>\n",
       "      <td>296586.0</td>\n",
       "      <td>10.90</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.45</td>\n",
       "      <td>40.87</td>\n",
       "      <td>26.61</td>\n",
       "      <td>2982930.21</td>\n",
       "      <td>72991.00</td>\n",
       "      <td>2.910573e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NICO</td>\n",
       "      <td>1740.00</td>\n",
       "      <td>1739.99</td>\n",
       "      <td>1514.17</td>\n",
       "      <td>1740.00</td>\n",
       "      <td>1740.01</td>\n",
       "      <td>1740.00</td>\n",
       "      <td>106674.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>1.15</td>\n",
       "      <td>3.97</td>\n",
       "      <td>25.20</td>\n",
       "      <td>11.66</td>\n",
       "      <td>1814891.51</td>\n",
       "      <td>72009.00</td>\n",
       "      <td>1.043041e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NITL</td>\n",
       "      <td>1454.01</td>\n",
       "      <td>1454.00</td>\n",
       "      <td>1454.01</td>\n",
       "      <td>1672.10</td>\n",
       "      <td>1454.00</td>\n",
       "      <td>1454.01</td>\n",
       "      <td>33406.0</td>\n",
       "      <td>11.10</td>\n",
       "      <td>0.76</td>\n",
       "      <td>15.16</td>\n",
       "      <td>6.60</td>\n",
       "      <td>2.66</td>\n",
       "      <td>196291.35</td>\n",
       "      <td>29759.48</td>\n",
       "      <td>1.350000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OMU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2500.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2500.06</td>\n",
       "      <td>2500.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.64</td>\n",
       "      <td>3.31</td>\n",
       "      <td>6.13</td>\n",
       "      <td>16.33</td>\n",
       "      <td>2.17</td>\n",
       "      <td>42375.51</td>\n",
       "      <td>2595.65</td>\n",
       "      <td>1.694980e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PCL</td>\n",
       "      <td>6600.00</td>\n",
       "      <td>6600.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6600.00</td>\n",
       "      <td>6600.00</td>\n",
       "      <td>6600.00</td>\n",
       "      <td>3394.0</td>\n",
       "      <td>11.25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>8.15</td>\n",
       "      <td>12.27</td>\n",
       "      <td>2.27</td>\n",
       "      <td>793688.41</td>\n",
       "      <td>64673.00</td>\n",
       "      <td>1.202558e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>STANDARD</td>\n",
       "      <td>6497.59</td>\n",
       "      <td>6497.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6497.30</td>\n",
       "      <td>6497.78</td>\n",
       "      <td>6497.47</td>\n",
       "      <td>4263.0</td>\n",
       "      <td>161.93</td>\n",
       "      <td>2.49</td>\n",
       "      <td>1.13</td>\n",
       "      <td>88.27</td>\n",
       "      <td>29.35</td>\n",
       "      <td>7623746.71</td>\n",
       "      <td>86365.00</td>\n",
       "      <td>1.173341e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SUNBIRD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>460.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>460.02</td>\n",
       "      <td>460.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>2.83</td>\n",
       "      <td>8.83</td>\n",
       "      <td>11.33</td>\n",
       "      <td>1.72</td>\n",
       "      <td>120333.22</td>\n",
       "      <td>10624.63</td>\n",
       "      <td>2.615826e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TNM</td>\n",
       "      <td>35.90</td>\n",
       "      <td>35.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.88</td>\n",
       "      <td>35.88</td>\n",
       "      <td>35.89</td>\n",
       "      <td>1961570.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.43</td>\n",
       "      <td>41.17</td>\n",
       "      <td>7.99</td>\n",
       "      <td>414213.68</td>\n",
       "      <td>10060.00</td>\n",
       "      <td>1.154120e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     counter  daily_range_high  daily_range_low      buy     sell  \\\n",
       "0     AIRTEL            138.83           138.82     0.00   138.83   \n",
       "1        BHL               NaN              NaN    15.01     0.00   \n",
       "2       FDHB            619.93           619.87   619.84   619.87   \n",
       "3      FMBCH           1650.41          1650.40  1650.41     0.00   \n",
       "4       ICON             17.95            17.95     0.00    17.95   \n",
       "5     ILLOVO           1791.42          1791.42  1791.42     0.00   \n",
       "6      MPICO               NaN              NaN    19.51     0.00   \n",
       "7        NBM           8200.00          7919.95  7919.95  7919.94   \n",
       "8        NBS           1024.89          1024.83  1024.84  1024.85   \n",
       "9       NICO           1740.00          1739.99  1514.17  1740.00   \n",
       "10      NITL           1454.01          1454.00  1454.01  1672.10   \n",
       "11       OMU               NaN              NaN  2500.06     0.00   \n",
       "12       PCL           6600.00          6600.00     0.00  6600.00   \n",
       "13  STANDARD           6497.59          6497.30     0.00  6497.30   \n",
       "14   SUNBIRD               NaN              NaN   460.02     0.00   \n",
       "15       TNM             35.90            35.87     0.00    35.88   \n",
       "\n",
       "    previous_closing_price  today_closing_price  volume_traded  dividend_mk  \\\n",
       "0                   138.84               138.82        53640.0         2.00   \n",
       "1                    15.00                15.00            0.0         0.00   \n",
       "2                   619.97               619.90       133897.0         4.73   \n",
       "3                  1650.40              1650.41         8900.0         3.64   \n",
       "4                    17.95                17.95        42822.0         0.29   \n",
       "5                  1791.41              1791.42         1440.0         5.00   \n",
       "6                    19.51                19.51            0.0         0.43   \n",
       "7                  7919.93              8196.77       122414.0       126.35   \n",
       "8                  1024.91              1024.86       296586.0        10.90   \n",
       "9                  1740.01              1740.00       106674.0        20.00   \n",
       "10                 1454.00              1454.01        33406.0        11.10   \n",
       "11                 2500.06              2500.06            0.0        82.64   \n",
       "12                 6600.00              6600.00         3394.0        11.25   \n",
       "13                 6497.78              6497.47         4263.0       161.93   \n",
       "14                  460.02               460.02            0.0        13.00   \n",
       "15                   35.88                35.89      1961570.0         0.00   \n",
       "\n",
       "    dividend_yield_pct  earnings_yield_pct  pe_ratio  pbv_ratio  \\\n",
       "0                 1.44                2.80     35.74      47.55   \n",
       "1                 0.00               -1.55    -64.36       1.36   \n",
       "2                 0.76                1.73     57.76      43.92   \n",
       "3                 0.22                2.91     34.31      12.33   \n",
       "4                 1.62               20.37      4.91       0.82   \n",
       "5                 0.28                1.77     56.47       8.59   \n",
       "6                 2.20               19.05      5.25       0.69   \n",
       "7                 1.54                2.67     37.42      14.26   \n",
       "8                 1.06                2.45     40.87      26.61   \n",
       "9                 1.15                3.97     25.20      11.66   \n",
       "10                0.76               15.16      6.60       2.66   \n",
       "11                3.31                6.13     16.33       2.17   \n",
       "12                0.17                8.15     12.27       2.27   \n",
       "13                2.49                1.13     88.27      29.35   \n",
       "14                2.83                8.83     11.33       1.72   \n",
       "15                0.00                2.43     41.17       7.99   \n",
       "\n",
       "    market_capitalization_mkmn  profit_after_tax_mkmn  num_shares_issue  \n",
       "0                   1527020.00               42722.11      1.100000e+10  \n",
       "1                     88173.82               -1370.11      5.878255e+09  \n",
       "2                   4277949.27               74063.00      6.901031e+09  \n",
       "3                   4057120.38              118254.74      2.458250e+09  \n",
       "4                    119906.00               24424.49      6.680000e+09  \n",
       "5                   1278078.55               22632.00      7.134444e+08  \n",
       "6                     44834.91                8540.17      2.298047e+09  \n",
       "7                   3827332.06              102283.00      4.669317e+08  \n",
       "8                   2982930.21               72991.00      2.910573e+09  \n",
       "9                   1814891.51               72009.00      1.043041e+09  \n",
       "10                   196291.35               29759.48      1.350000e+08  \n",
       "11                    42375.51                2595.65      1.694980e+07  \n",
       "12                   793688.41               64673.00      1.202558e+08  \n",
       "13                  7623746.71               86365.00      1.173341e+09  \n",
       "14                   120333.22               10624.63      2.615826e+08  \n",
       "15                   414213.68               10060.00      1.154120e+10  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3248555d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2769006"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(df.volume_traded.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ser_no', 'daily_range_high', 'daily_range_low', \n",
    "        'counter', 'buy', 'sell', 'previous_closing_price', \n",
    "        'today_closing_price', 'volume_traded', 'dividend_mk', 'dividend_yield_pct',\n",
    "        'earnings_yield_pct', 'pe_ratio', 'pbv_ratio', 'market_capitalization_mkmn',\n",
    "        'profit_after_tax_mkmn', 'num_shares_issue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeab88e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Rename the DataFrame columns to use our predefined column names\n",
    "# First, let's ensure we're using the right DataFrame and columns\n",
    "# Our dataframe 'df' already exists, so we'll use it and rename columns to match 'cols'\n",
    "\n",
    "# Check if columns match in length before renaming\n",
    "if len(df.columns) == len(cols):\n",
    "    df.columns = cols\n",
    "else:\n",
    "    # Handle the case where column counts don't match\n",
    "    print(f\"Column count mismatch: df has {len(df.columns)} columns, cols has {len(cols)} columns\")\n",
    "    # Assign columns up to the minimum length to avoid errors\n",
    "    min_length = min(len(df.columns), len(cols))\n",
    "    df.columns = cols[:min_length] + list(df.columns[min_length:]) if len(df.columns) > min_length else cols\n",
    "    \n",
    "# Preview the DataFrame with new column names\n",
    "print(\"DataFrame with renamed columns:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5baf347",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0161f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first table from the uploaded PDF and save it to CSV\n",
    "\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "# from caas_jupyter_tools import display_dataframe_to_user\n",
    "\n",
    "\n",
    "def clean_cell(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    # Normalize whitespace and remove stray newlines\n",
    "    x = re.sub(r'\\s+', ' ', str(x)).strip()\n",
    "    # Replace weird unicode minus or similar artifacts if any\n",
    "    x = x.replace('–', '-').replace('—', '-')\n",
    "    return x if x != '' else None\n",
    "\n",
    "def header_from_rows(rows):\n",
    "    \"\"\"\n",
    "    Pick the first row that looks like a header (most non-empty cells).\n",
    "    Return (header, start_index_for_data)\n",
    "    \"\"\"\n",
    "    best_idx, best_count = None, -1\n",
    "    for i, r in enumerate(rows[:5]):  # look at the first few rows\n",
    "        non_empty = sum(1 for c in r if c is not None and str(c).strip() != '')\n",
    "        if non_empty > best_count:\n",
    "            best_count = non_empty\n",
    "            best_idx = i\n",
    "    return rows[best_idx], best_idx + 1\n",
    "\n",
    "first_table_df = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function\n",
    "df = extract_first_table(pdf_path, out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca632d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(FILE_PDF) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        # Try multiple strategies to improve table detection\n",
    "        strategies = [\n",
    "            dict(vertical_strategy=\"lines\", horizontal_strategy=\"lines\"),\n",
    "            dict(vertical_strategy=\"lines_strict\", horizontal_strategy=\"lines_strict\"),\n",
    "            dict(vertical_strategy=\"text\", horizontal_strategy=\"text\"),\n",
    "        ]\n",
    "        tables = []\n",
    "        for ts in strategies:\n",
    "            try:\n",
    "                t = page.extract_tables(table_settings=ts)\n",
    "                if t:\n",
    "                    tables.extend(t)\n",
    "            except Exception as e:\n",
    "                # Continue trying with other strategies\n",
    "                pass\n",
    "        if tables:\n",
    "            # Use the first detected table on the first page that has any tables\n",
    "            raw = tables[0]\n",
    "            # Clean cells\n",
    "            rows = [[clean_cell(c) for c in row] for row in raw]\n",
    "            # Drop completely empty rows\n",
    "            rows = [row for row in rows if any(cell is not None for cell in row)]\n",
    "            if not rows:\n",
    "                continue\n",
    "            header, start_idx = header_from_rows(rows)\n",
    "            # If header has duplicates or Nones, generate generic names\n",
    "            cols = []\n",
    "            seen = {}\n",
    "            for i, name in enumerate(header):\n",
    "                name = name or f\"col_{i+1}\"\n",
    "                name = re.sub(r'\\s+', ' ', name).strip()\n",
    "                if name in seen:\n",
    "                    seen[name] += 1\n",
    "                    name = f\"{name}_{seen[name]}\"\n",
    "                else:\n",
    "                    seen[name] = 1\n",
    "                cols.append(name)\n",
    "            data = rows[start_idx:]\n",
    "            # Normalize row lengths to header length\n",
    "            norm_data = []\n",
    "            for r in data:\n",
    "                if len(r) < len(cols):\n",
    "                    r = r + [None] * (len(cols) - len(r))\n",
    "                elif len(r) > len(cols):\n",
    "                    r = r[:len(cols)]\n",
    "                norm_data.append(r)\n",
    "            df = pd.DataFrame(norm_data, columns=cols)\n",
    "            # Drop rows that are entirely NaN\n",
    "            df = df.dropna(how=\"all\")\n",
    "            # Keep a copy of the first table only\n",
    "            first_table_df = df\n",
    "            break\n",
    "\n",
    "# If we didn't manage to extract a table, create an empty placeholder DataFrame\n",
    "if first_table_df is None:\n",
    "    first_table_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f43fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bcc499",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_table_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14f9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
